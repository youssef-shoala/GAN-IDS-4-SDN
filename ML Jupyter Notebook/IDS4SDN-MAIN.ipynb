{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679c8710-8eb6-44b0-8058-ab9e71cc2b96",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dab214-332e-46fd-91d2-9bd408b1ec11",
   "metadata": {},
   "source": [
    "#todo   The environment is set up with anaconda take note of conda installation and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47899b4-4b65-4874-ade8-ffd57eace95b",
   "metadata": {},
   "source": [
    "## Select Device Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c6879-641e-4d1e-af4e-f8dc8e14b067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"NOTE : GPU in use\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"NOTE : CPU in use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe85fc-3454-4c40-b308-2d0200253283",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa16d05-62cf-4396-a4db-09d2130196a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bab0a-999c-42a6-8e26-2364c4e14e98",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086925e-7465-4705-93ff-81d7032ca68a",
   "metadata": {},
   "source": [
    "## Get Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be395c01-07f5-47f2-a67a-49efe41d2f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data from file\n",
    "gan_train_data = pd.read_csv('KDDTrain+TopRowColNames.txt', header=None)\n",
    "\n",
    "#drop duplicates\n",
    "gan_train_data.drop_duplicates()\n",
    "\n",
    "\n",
    "print(gan_train_data.columns)\n",
    "print(gan_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e58ed-93a3-47e3-9028-784ebd5bc2b0",
   "metadata": {},
   "source": [
    "## Prepare Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f970d-1dc1-4b29-a560-a69ed20c2749",
   "metadata": {},
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a53d9-35cc-4997-88f6-b9dac1dc39d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all cols that have non float data type\n",
    "for col in gan_train_data.columns:\n",
    "    if gan_train_data.dtypes[col] != type(0.1): \n",
    "        print(f\"{col} of type {gan_train_data.dtypes[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366cb717-f308-4d39-a0aa-3a036d3b1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate distinct values in col and add to dict \n",
    "def get_unique_keys_and_enumerate_dict(df, col_name): \n",
    "    # Input: dataframe df and target col_name\n",
    "    # Output: dict in the form {'tcp': 0, 'udp': 1, 'icmp': 2}\n",
    "    value_list = []\n",
    "    for value in df[col_name]: \n",
    "        if value not in value_list: \n",
    "            value_list.append(value)\n",
    "            #print(value)\n",
    "    \n",
    "    dict_items = {value:index for index, value in enumerate(value_list)}\n",
    "    print(dict_items)\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    return dict_items\n",
    "\n",
    "#get_unique_keys_and_enumerate_dict(gan_train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bd1d9-7421-4b68-88b2-d625429b9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Col Dictionaries\n",
    "attack_types = get_unique_keys_and_enumerate_dict(gan_train_data, 41)\n",
    "attack_types_binary = attack_types.copy()\n",
    "for key in attack_types_binary:\n",
    "    if key != 'normal':\n",
    "        attack_types_binary[key] = 1\n",
    "print(attack_types_binary)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "protocol_types = get_unique_keys_and_enumerate_dict(gan_train_data, 1)\n",
    "service = get_unique_keys_and_enumerate_dict(gan_train_data, 2)\n",
    "flag =  get_unique_keys_and_enumerate_dict(gan_train_data, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df64a4-6548-4630-af65-893a6872f69c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize protocol_types\n",
    "for value in gan_train_data[1]: \n",
    "    #print(value)\n",
    "    #print(protocol_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        gan_train_data[1] = gan_train_data[1].replace(value, protocol_types[value])\n",
    "        #print(f\"Change {value} to {protocol_types[value]}\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8302f68-d6c7-4d4d-9af6-6018bda639c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize service\n",
    "for value in gan_train_data[2]: \n",
    "    #print(value)\n",
    "    #print(service[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        gan_train_data[2] = gan_train_data[2].replace(value, service[value])\n",
    "        #print(f\"Change {value} to {service[value]}\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640be0cd-1f15-4c79-9b77-cd65b4ab8e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize flag\n",
    "for value in gan_train_data[3]: \n",
    "    #print(value)\n",
    "    #print(flag[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        gan_train_data[3] = gan_train_data[3].replace(value, flag[value])\n",
    "        #print(f\"Change {value} to {flag[value]}\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660c613-c11d-44b3-98b5-37cc244e1e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize class\n",
    "normal_traffic_count = 0\n",
    "attack_traffic_count = 0\n",
    "for value in gan_train_data[41]: \n",
    "    #print(value)\n",
    "    #print(attack_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        if value == 'normal': \n",
    "            normal_traffic_count+=1\n",
    "        else:\n",
    "            attack_traffic_count+=1\n",
    "        gan_train_data[41] = gan_train_data[41].replace(value, attack_types[value])\n",
    "\n",
    "        #print(f\"Change {value} to {attack_types[value]}\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d494a08-fb52-423e-a009-e2119e0807c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_traffic_count, attack_traffic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16413e0f-f111-4fa7-89cb-faa9d5430e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce float64 datatype for all columns data\n",
    "for col in gan_train_data.columns: \n",
    "    gan_train_data[col] = gan_train_data[col].astype('float64')\n",
    "gan_train_data.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d79bec-0131-4423-aac3-26642866c86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display new data form\n",
    "print(gan_train_data.dtypes)\n",
    "print(gan_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4541ae-4d07-433b-8e21-cadd7496b4e8",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8be86d-c98d-444a-8633-5e3461ea2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_i = (x_i - Min) / (Max - Min)\n",
    "# Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "exclude_cols = []\n",
    "for col in gan_train_data:\n",
    "    if col not in exclude_cols:\n",
    "        gan_train_data[col] = scaler.fit_transform(gan_train_data[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66611572-283e-4752-9bc9-5907f2352442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display new data form\n",
    "for col in gan_train_data.columns: \n",
    "    min_val = gan_train_data[col].min()\n",
    "    max_val = gan_train_data[col].max()\n",
    "    print(f\"Min: {min_val}, Max: {max_val}\")\n",
    "\n",
    "print(gan_train_data)\n",
    "gan_train_data.to_csv('gan_train_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d3355-3abb-4797-bb73-54b9abfc09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_traffic_count = 0\n",
    "attack_traffic_count = 0\n",
    "normal_threshold = .02 \n",
    "for value in gan_train_data[41]: \n",
    "    if value >= normal_threshold: \n",
    "        attack_traffic_count+=1\n",
    "    else:\n",
    "        normal_traffic_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d72a2-24ff-4d40-ac44-2f8093c2fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_traffic_count, attack_traffic_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b52e4-9038-4552-9343-208c0fca5edc",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a49a8-1853-4f26-a011-a853fd2a2c5a",
   "metadata": {},
   "source": [
    "### Create SDV Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9d6c5-416d-4f66-8e17-1074d6000f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get metadata from dataframes\n",
    "if 'gan_train_data.txt' in os.listdir(): \n",
    "    gan_train_data = pd.read_csv('gan_train_data.txt')\n",
    "    print('loaded gan training data: ')\n",
    "\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "print(type(gan_train_data))\n",
    "gan_train_metadata = SingleTableMetadata()\n",
    "for col in gan_train_data:\n",
    "    gan_train_metadata.add_column(\n",
    "        column_name=str(col),\n",
    "        sdtype='numerical',\n",
    "        computer_representation='Float')\n",
    "    gan_train_data = gan_train_data.rename(columns={col:str(col)})\n",
    "#print(gan_train_metadata.to_dict())\n",
    "print(gan_train_data.columns)\n",
    "#gan_train_metadata.detect_from_dataframe(gan_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404c989-a2ac-4ca0-b202-1374f2ab857d",
   "metadata": {},
   "source": [
    "### CTGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853df5b7-831f-428e-886c-aa11da646a7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: RETRAIN GANS WITH DROPPED DUPES\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import os\n",
    "\n",
    "# Fit Model only if model not found in directory\n",
    "if 'ctGanSynthesizer.pkl' in os.listdir():\n",
    "    ctGanSynthesizer = CTGANSynthesizer.load(filepath='ctGanSynthesizer.pkl')\n",
    "    print('CT GAN Synthesizer loaded into ctGanSynthesizer variable')\n",
    "    print(ctGanSynthesizer.get_parameters())\n",
    "    print(ctGanSynthesizer.get_metadata())\n",
    "else:\n",
    "    ctGanSynthesizer = CTGANSynthesizer(gan_train_metadata, verbose=True)\n",
    "    ctGanSynthesizer.fit(gan_train_data)\n",
    "    ctGanSynthesizer.save(filepath='ctGanSynthesizer.pkl')\n",
    "    print(ctGanSynthesizer.get_parameters())\n",
    "    print(ctGanSynthesizer.get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf78447-94c9-45b5-8284-508b07088162",
   "metadata": {},
   "source": [
    "### CouplaGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d65649-bd8d-4e68-ab63-e80508df4dc2",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdv.single_table import CopulaGANSynthesizer\n",
    "import os\n",
    "\n",
    "# Fit Model only if model not found in directory\n",
    "if 'copulaGanSynthesizer.pkl' in os.listdir():\n",
    "    copulaGanSynthesizer = CopulaGANSynthesizer.load(filepath='copulaGanSynthesizer.pkl')\n",
    "    print('Coupla GAN Synthesizer loaded into copulaGanSynthesizer variable')\n",
    "    print(copulaGanSynthesizer.get_parameters())\n",
    "    print(copulaGanSynthesizer.get_metadata())\n",
    "else:\n",
    "    copulaGanSynthesizer = CopulaGANSynthesizer(gan_train_metadata, verbose=True)\n",
    "    copulaGanSynthesizer.fit(gan_train_data)\n",
    "    copulaGanSynthesizer.save(filepath='copulaGanSynthesizer.pkl')\n",
    "    print(copulaGanSynthesizer.get_parameters())\n",
    "    print(copulaGanSynthesizer.get_metadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188db270-b6fc-4c2e-bebb-cd788b9af7b1",
   "metadata": {},
   "source": [
    "# Train Discriminators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d588096-7a8c-4ae8-bd71-719f2e19e7ac",
   "metadata": {},
   "source": [
    "## Get and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229e3c6-04a0-4346-b273-11bf2f31a356",
   "metadata": {},
   "source": [
    "### Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee0422-4b04-4f44-8d3f-d42b4772e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Real Data\n",
    "if 'real_train_data.txt' not in os.listdir():\n",
    "    real_train_data = gan_train_data\n",
    "    print('real_train_data')\n",
    "    print(real_train_data)\n",
    "else: \n",
    "    real_train_data = pd.read_csv('real_train_data.txt')\n",
    "    print('loaded real_train_data')\n",
    "    print(real_train_data)\n",
    "\n",
    "# Synthetic Data\n",
    "### Possible dataset: \n",
    "### ct vs coupla\n",
    "### size of data vs stats of data (ratio attack_type:normal packets)\n",
    "### separating each attack type into its own discriminator?\n",
    "\n",
    "# ctGanSynthesizer\n",
    "if 'ctGanSynthesizer_synthetic_train_data.txt' not in os.listdir():\n",
    "    ctGanSynthesizer_synthetic_train_data = ctGanSynthesizer.sample(500000)\n",
    "    ctGanSynthesizer_synthetic_train_data.drop_duplicates()\n",
    "    print('\\n\\n ctGanSynthesizer_synthetic_train_data')\n",
    "    print(ctGanSynthesizer_synthetic_train_data)\n",
    "\n",
    "else: \n",
    "    ctGanSynthesizer_synthetic_train_data = pd.read_csv('ctGanSynthesizer_synthetic_train_data.txt')\n",
    "    print('\\n\\n loaded ctGanSynthesizer_synthetic_train_data')\n",
    "    print(ctGanSynthesizer_synthetic_train_data)\n",
    "\n",
    "# couplaGanSynthesizer (not working, still in beta, see below block)\n",
    "if 'couplaGanSynthesizer_synthetic_train_data.txt' not in os.listdir():\n",
    "    couplaGanSynthesizer_synthetic_train_data = copulaGanSynthesizer.sample(500000)\n",
    "    couplaGanSynthesizer_synthetic_train_data.drop_duplicates()\n",
    "    print('\\n\\n couplaGanSynthesizer_synthetic_train_data')\n",
    "    print(couplaGanSynthesizer_synthetic_train_data)\n",
    "\n",
    "else: \n",
    "    couplaGanSynthesizer_synthetic_train_data = pd.read_csv('couplaGanSynthesizer_synthetic_train_data.txt')\n",
    "    print('\\n\\n loaded couplaGanSynthesizer_synthetic_train_data')\n",
    "    print(couplaGanSynthesizer_synthetic_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083789f5-b822-4c75-9b5c-d915a87239ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#real_train_data.drop(columns=['Unnamed: 0'])\n",
    "#ctGanSynthesizer_synthetic_train_data.drop(columns=['Unnamed: 0'])\n",
    "#couplaGanSynthesizer_synthetic_train_data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "#print('\\n\\n compare')\n",
    "#for i in range(5): \n",
    "#    print(f'row:{i}')\n",
    "#    for j in range(42):\n",
    "#        print(f'real: {real_train_data.iloc[i][j]}, synthetic: {synthetic_train_data.iloc[i][j]}')\n",
    "\n",
    "\n",
    "\n",
    "#get_unique_keys_and_enumerate_dict(real_train_data, '41')\n",
    "\n",
    "### IMPORTANT: couplaGAN Synthesizer not creating similar enough data. Seems that normal class has dissapeared from it\n",
    "get_unique_keys_and_enumerate_dict(ctGanSynthesizer.sample(500), '41')\n",
    "dict1 = get_unique_keys_and_enumerate_dict(copulaGanSynthesizer.sample(500), '41')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da82b0-fe5a-42ea-8e04-884892173b01",
   "metadata": {},
   "source": [
    "### Binary classification - attack method no longer specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153abbb-67ac-484d-97b0-869350835acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Data\n",
    "if 'real_train_data.txt' not in os.listdir():\n",
    "    normal_threshold = .02 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in real_train_data[41]: \n",
    "        #print(value)\n",
    "        #print(attack_types[value])\n",
    "        if value >= normal_threshold: \n",
    "            real_train_data[41] = real_train_data[41].replace(value, 1)\n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            real_train_data[41] = real_train_data[41].replace(value, 0)\n",
    "            normal_traffic_count += 1\n",
    "    print('real_train_data classes:')\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n",
    "    print(real_train_data[41])\n",
    "    print(normal_traffic_count, attack_traffic_count)\n",
    "    real_train_data.to_csv('real_train_data.txt')\n",
    "else:\n",
    "    normal_threshold = .02 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in real_train_data['41']: \n",
    "        if value >= normal_threshold: \n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            normal_traffic_count += 1\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c831e-9459-44ee-81c0-d9941ecf655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct GAN Synthetic Data\n",
    "if 'ctGanSynthesizer_synthetic_train_data.txt' not in os.listdir():\n",
    "    normal_threshold = .02 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in ctGanSynthesizer_synthetic_train_data['41']: \n",
    "        #print(value)\n",
    "        #print(attack_types[value])\n",
    "        if value >= normal_threshold: \n",
    "            ctGanSynthesizer_synthetic_train_data['41'] = ctGanSynthesizer_synthetic_train_data['41'].replace(value, 1)\n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            ctGanSynthesizer_synthetic_train_data['41'] = ctGanSynthesizer_synthetic_train_data['41'].replace(value, 0)\n",
    "            normal_traffic_count += 1\n",
    "    print('\\n\\nctGanSynthesizer_synthetic_train_data classes:')\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n",
    "    print(ctGanSynthesizer_synthetic_train_data['41'])\n",
    "    ctGanSynthesizer_synthetic_train_data.to_csv('ctGanSynthesizer_synthetic_train_data.txt') \n",
    "else:\n",
    "    normal_threshold = .02 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in ctGanSynthesizer_synthetic_train_data['41']: \n",
    "        if value >= normal_threshold: \n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            normal_traffic_count += 1\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8e2aa-a5cd-402e-982d-b687ea5764bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupla GAN Synthetic Data \n",
    "if 'couplaGanSynthesizer_synthetic_train_data.txt' not in os.listdir():\n",
    "    normal_threshold = .06 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in couplaGanSynthesizer_synthetic_train_data['41']: \n",
    "        #print(value)|\n",
    "        #print(attack_types[value])\n",
    "        if value >= normal_threshold: \n",
    "            couplaGanSynthesizer_synthetic_train_data['41'] = couplaGanSynthesizer_synthetic_train_data['41'].replace(value, 1)\n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            couplaGanSynthesizer_synthetic_train_data['41'] = couplaGanSynthesizer_synthetic_train_data['41'].replace(value, 0)\n",
    "            normal_traffic_count += 1\n",
    "    print('\\n\\ncouplaGanSynthesizer_synthetic_train_data classes:')\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n",
    "    print(couplaGanSynthesizer_synthetic_train_data['41'])\n",
    "    #print(normal_traffic_count, attack_traffic_count)\n",
    "    couplaGanSynthesizer_synthetic_train_data.to_csv('couplaGanSynthesizer_synthetic_train_data.txt') \n",
    "else:\n",
    "    normal_threshold = .02 \n",
    "    normal_traffic_count = 0\n",
    "    attack_traffic_count = 0\n",
    "    for value in couplaGanSynthesizer_synthetic_train_data['41']: \n",
    "        if value >= normal_threshold: \n",
    "            attack_traffic_count += 1\n",
    "        else:\n",
    "            normal_traffic_count += 1\n",
    "    print(f'normal_traffic: {normal_traffic_count}, attack traffic: {attack_traffic_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89bb2d-0ac1-4b4d-a13b-cb76834f498f",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa99285-db2b-4b0f-b5c1-60ee8748ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all curr datasets\n",
    "real_train_data_rows = 125973\n",
    "ctGanSynthesizer_synthetic_train_data_rows = 500000\n",
    "couplaGanSynthesizer_synthetic_train_data_rows = 500000\n",
    "\n",
    "real_train_data_columns = 42 #does not include class column = total_col - 1\n",
    "ctGanSynthesizer_synthetic_train_data_columns = 42\n",
    "couplaGanSynthesizer_synthetic_train_data_columns = 42\n",
    "\n",
    "train_dataframes = [real_train_data, ctGanSynthesizer_synthetic_train_data, couplaGanSynthesizer_synthetic_train_data]\n",
    "\n",
    "# Init target \n",
    "real_train_data_training_targets = torch.zeros(1, real_train_data_rows, dtype=torch.double)\n",
    "ctGanSynthesizer_synthetic_train_data_training_targets = torch.zeros(1, ctGanSynthesizer_synthetic_train_data_rows, dtype=torch.double)\n",
    "couplaGanSynthesizer_synthetic_train_data_training_targets = torch.zeros(1, couplaGanSynthesizer_synthetic_train_data_rows, dtype=torch.double)\n",
    "target_tensors = [real_train_data_training_targets, ctGanSynthesizer_synthetic_train_data_training_targets, couplaGanSynthesizer_synthetic_train_data_training_targets]\n",
    "\n",
    "# List of features intended to be dropped\n",
    "# keep: 1, 2, 3, 11, 22, 24, 25, 28, 31, 32, 37, 38\n",
    "real_train_data_cols_to_drop = [42, 40, 39, 36, 35, 34, 33, 30, 29, 27, 26, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 0]\n",
    "ctGanSynthesizer_synthetic_train_data_cols_to_drop = [42, 40, 39, 36, 35, 34, 33, 30, 29, 27, 26, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 0]\n",
    "couplaGanSynthesizer_synthetic_train_data_cols_to_drop = [42, 40, 39, 36, 35, 34, 33, 30, 29, 27, 26, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 0]\n",
    "cols_to_drop = [real_train_data_cols_to_drop, ctGanSynthesizer_synthetic_train_data_cols_to_drop, couplaGanSynthesizer_synthetic_train_data_cols_to_drop]\n",
    "\n",
    "# Init data tensors\n",
    "real_train_data_training_samples = torch.zeros(real_train_data_columns-len(cols_to_drop[0]), real_train_data_rows, dtype=torch.double)\n",
    "ctGanSynthesizer_synthetic_train_data_training_samples = torch.zeros(ctGanSynthesizer_synthetic_train_data_columns-len(cols_to_drop[1]), ctGanSynthesizer_synthetic_train_data_rows, dtype=torch.double)\n",
    "couplaGanSynthesizer_synthetic_train_data_training_samples = torch.zeros(couplaGanSynthesizer_synthetic_train_data_columns-len(cols_to_drop[2]), couplaGanSynthesizer_synthetic_train_data_rows, dtype=torch.double)\n",
    "sample_tensors = [real_train_data_training_samples, ctGanSynthesizer_synthetic_train_data_training_samples, couplaGanSynthesizer_synthetic_train_data_training_samples]\n",
    "\n",
    "i=0\n",
    "for df in train_dataframes:\n",
    "    # insert class values into initialized training_targets tensor\n",
    "    class_col_val = 41\n",
    "    try:\n",
    "        numpy_array = df[class_col_val].values\n",
    "    except:\n",
    "        class_col_val = '41'\n",
    "        cols_to_drop[i] = [str(j) for j in cols_to_drop[i]]\n",
    "        numpy_array = df[class_col_val].values\n",
    "    target_tensors[i] = torch.from_numpy(numpy_array).double()\n",
    "    # Drop classification and create target tensor\n",
    "    df_after_drop = df.drop(columns=[class_col_val])\n",
    "    # Drop rest of cols in cols_to_drop\n",
    "    df_after_drop = df_after_drop.drop(columns=cols_to_drop[i])\n",
    "    # insert remaining col values into initialized training_samples tensor\n",
    "    numpy_array = df_after_drop.values\n",
    "    sample_tensors[i] = torch.from_numpy(numpy_array).double()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b69db-c8d8-4b34-8bba-64ac75a1958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_tensors)): \n",
    "    print(i, '-----------------------------------------------')\n",
    "    print(target_tensors[i].size())\n",
    "    print(target_tensors[i])\n",
    "\n",
    "    print(sample_tensors[i].size())\n",
    "    print(sample_tensors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b03eb6-5dbe-46f2-9f34-e411d89ccd20",
   "metadata": {},
   "source": [
    "## DNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b20b2a-016a-4b80-9569-e88ad098443c",
   "metadata": {},
   "source": [
    "### Create DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95472faa-3079-40a6-91fb-caa50eb77d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Model Architecture\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(real_train_data_columns-len(cols_to_drop[0]), 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.sigmoid(self.dense1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.sigmoid(self.dense2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense3(x)\n",
    "        #debugging(x)\n",
    "        return x\n",
    "def debugging(x):\n",
    "    print(x)\n",
    "\n",
    "# create discriminator instance\n",
    "dnn_discriminator_real_data = SimpleDNN()\n",
    "dnn_discriminator_ctGan_synthetic_data = SimpleDNN()\n",
    "dnn_discriminator_couplaGan_synthetic_data = SimpleDNN()\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "dnn_discriminator_real_data_criterion = nn.BCEWithLogitsLoss()\n",
    "dnn_discriminator_ctGan_synthetic_data_criterion = nn.BCEWithLogitsLoss()\n",
    "dnn_discriminator_couplaGan_synthetic_data_criterion = nn.BCEWithLogitsLoss()\n",
    "dnn_discriminator_real_data_optimizer = optim.Adam(dnn_discriminator_real_data.parameters())\n",
    "dnn_discriminator_ctGan_synthetic_data_optimizer = optim.Adam(dnn_discriminator_ctGan_synthetic_data.parameters())\n",
    "dnn_discriminator_couplaGan_synthetic_data_optimizer = optim.Adam(dnn_discriminator_couplaGan_synthetic_data.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775aa90-7c47-4337-8f32-3dcecd2c839c",
   "metadata": {},
   "source": [
    "### Train Model on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2430557-82bd-49e8-92be-c9b5a750c46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train DNN on Real Data\n",
    "if 'dnn_discriminator_real_data' in os.listdir():\n",
    "    dnn_discriminator_real_data = SimpleDNN()\n",
    "    dnn_discriminator_real_data.load_state_dict(torch.load('dnn_discriminator_real_data'))\n",
    "    dnn_discriminator_real_data.eval()\n",
    "else:\n",
    "    dnn_discriminator_real_data.train()\n",
    "    epochs = 100000\n",
    "    loss_limit = .17\n",
    "    input_data = sample_tensors[0].double()\n",
    "    target = target_tensors[0].double()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        # Forward pass\n",
    "        output = dnn_discriminator_real_data(input_data.float())\n",
    "        \n",
    "        # Calculate loss\n",
    "        target = target.view(-1, 1)\n",
    "        loss = dnn_discriminator_real_data_criterion(output, target)\n",
    "        print(f'curr_loss: {loss.item()}')\n",
    "        if loss.item() < loss_limit:\n",
    "            break\n",
    "        \n",
    "        # Backward pass\n",
    "        dnn_discriminator_real_data_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        dnn_discriminator_real_data_optimizer.step()\n",
    "    \n",
    "    torch.save(dnn_discriminator_real_data.state_dict(), 'dnn_discriminator_real_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe42bd-feda-48a1-9f32-45bfb3bff8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train DNN on ct Gan Synthetic Data\n",
    "if 'dnn_discriminator_ctGan_synthetic_data' in os.listdir():\n",
    "    dnn_discriminator_ctGan_synthetic_data = SimpleDNN()\n",
    "    dnn_discriminator_ctGan_synthetic_data.load_state_dict(torch.load('dnn_discriminator_ctGan_synthetic_data'))\n",
    "    dnn_discriminator_ctGan_synthetic_data.eval()\n",
    "else:\n",
    "    dnn_discriminator_ctGan_synthetic_data.train()\n",
    "    epochs = 100000\n",
    "    loss_limit = .17\n",
    "    input_data = sample_tensors[1].double()\n",
    "    target = target_tensors[1].double()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        # Forward pass\n",
    "        output = dnn_discriminator_ctGan_synthetic_data(input_data.float())\n",
    "        \n",
    "        # Calculate loss\n",
    "        target = target.view(-1, 1)\n",
    "        loss = dnn_discriminator_ctGan_synthetic_data_criterion(output, target)\n",
    "        print(f'curr_loss: {loss.item()}')\n",
    "        if loss.item() < loss_limit:\n",
    "            break\n",
    "        \n",
    "        # Backward pass\n",
    "        dnn_discriminator_ctGan_synthetic_data_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        dnn_discriminator_ctGan_synthetic_data_optimizer.step()\n",
    "    \n",
    "    torch.save(dnn_discriminator_ctGan_synthetic_data.state_dict(), 'dnn_discriminator_ctGan_synthetic_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90f7b7-c139-48fa-9fea-a14d4e26b656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train DNN on coupla Gan Synthetic Data\n",
    "if 'dnn_discriminator_couplaGan_synthetic_data' in os.listdir():\n",
    "    dnn_discriminator_couplaGan_synthetic_data = SimpleDNN()\n",
    "    dnn_discriminator_couplaGan_synthetic_data.load_state_dict(torch.load('dnn_discriminator_couplaGan_synthetic_data'))\n",
    "    dnn_discriminator_couplaGan_synthetic_data.eval()\n",
    "else:\n",
    "    dnn_discriminator_couplaGan_synthetic_data.train()\n",
    "    epochs = 100000\n",
    "    loss_limit = .17\n",
    "    input_data = sample_tensors[2].double()\n",
    "    target = target_tensors[2].double()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        # Forward pass\n",
    "        output = dnn_discriminator_couplaGan_synthetic_data(input_data.float())\n",
    "        \n",
    "        # Calculate loss\n",
    "        target = target.view(-1, 1)\n",
    "        loss = dnn_discriminator_couplaGan_synthetic_data_criterion(output, target)\n",
    "        print(f'curr_loss: {loss.item()}')\n",
    "        if loss.item() < loss_limit:\n",
    "            break\n",
    "        \n",
    "        # Backward pass\n",
    "        dnn_discriminator_couplaGan_synthetic_data_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        dnn_discriminator_couplaGan_synthetic_data_optimizer.step()\n",
    "    \n",
    "    torch.save(dnn_discriminator_couplaGan_synthetic_data.state_dict(), 'dnn_discriminator_couplaGan_synthetic_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0ff04-432c-4881-9add-cc29f9a8d190",
   "metadata": {},
   "source": [
    "## CNN (DOES NOT WORK DO NOT RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76213c2-d02e-452f-8f40-43b7bb981c89",
   "metadata": {},
   "source": [
    "### Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ad697-5441-4611-93ce-188c0ed05a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_channels = 16\n",
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# create discriminator instance\n",
    "cnn_discriminator_real_data = Simple1DCNN()\n",
    "cnn_discriminator_ctGan_synthetic_data = Simple1DCNN()\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "cnn_discriminator_real_data_criterion = nn.BCEWithLogitsLoss()\n",
    "cnn_discriminator_ctGan_synthetic_data_criterion = nn.BCEWithLogitsLoss()\n",
    "cnn_discriminator_real_data_optimizer = optim.Adam(cnn_discriminator_real_data.parameters())\n",
    "cnn_discriminator_ctGan_synthetic_data_optimizer = optim.Adam(cnn_discriminator_ctGan_synthetic_data.parameters())\n",
    "\n",
    "#create custom dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomCNNDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input = self.inputs[index].double()\n",
    "        target = self.targets[index].double()\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96f860-9ff1-4ee4-850c-e337b49cf18d",
   "metadata": {},
   "source": [
    "### Train Model on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b052d-a7da-456f-b68a-9d1e39782f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Train CNN on Real Data\n",
    "if 'cnn_discriminator_real_data' in os.listdir():\n",
    "    cnn_discriminator_real_data = Simple1DCNN()\n",
    "    cnn_discriminator_real_data.load_state_dict(torch.load('cnn_discriminator_real_data'))\n",
    "    cnn_discriminator_real_data.eval()\n",
    "else:\n",
    "    epochs = 100000\n",
    "    loss_limit = .17\n",
    "    batch_size = input_channels\n",
    "\n",
    "    cnn_discriminator_real_data = Simple1DCNN()\n",
    "    cnn_discriminator_real_data.train()\n",
    "    criterion = cnn_discriminator_real_data_criterion\n",
    "    optimizer = cnn_discriminator_real_data_optimizer\n",
    "\n",
    "    input_data = sample_tensors[0].double()\n",
    "    target = target_tensors[0].double()\n",
    "    \n",
    "    custom_dataset = CustomCNNDataset(input_data, target)\n",
    "    train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch}')\n",
    "        total_loss = 0\n",
    "        num_loss_items = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.double()\n",
    "            outputs = cnn_discriminator_real_data(inputs) \n",
    "            #targets = targets.view(-1, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss\n",
    "            num_loss_items += 1\n",
    "        \n",
    "        print(f'last batch avg loss = {total_loss/num_loss_items}')\n",
    "        if total_loss/num_loss_items < loss_limit: \n",
    "            break\n",
    "    torch.save(cnn_discriminator_real_data.state_dict(), 'cnn_discriminator_real_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677eb4d-5ca3-4236-86aa-08c90028d8ae",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c155f-de94-43fe-b26a-a042c40534b2",
   "metadata": {},
   "source": [
    "## Get Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c978b17-f257-47a5-b151-5efc93ae8b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data from file\n",
    "test_data = pd.read_csv('KDDTest+.txt', header=None) #, names=col_name_lis\n",
    "easy_test_data = pd.read_csv('KDDTest-21.txt', header=None)\n",
    "\n",
    "#drop duplicates\n",
    "test_data.drop_duplicates()\n",
    "easy_test_data.drop_duplicates()\n",
    "\n",
    "#print \n",
    "print(test_data)\n",
    "print(easy_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e438dd-56df-4e4f-baff-d7926cda9612",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e155a-dc9e-4c23-900a-dec324c34699",
   "metadata": {},
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2e57d-49f3-4766-90d3-32f3413ec686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Col Dictionaries\n",
    "#attack_types = get_unique_keys_and_enumerate_dict(gan_train_data, 41)\n",
    "#attack_types_binary = attack_types.copy()\n",
    "#for key in attack_types_binary:\n",
    "#    if key != 'normal':\n",
    "#        attack_types_binary[key] = 1\n",
    "#print(attack_types_binary)\n",
    "#print('\\n')\n",
    "#print('\\n')\n",
    "#print('\\n')\n",
    "\n",
    "#protocol_types = get_unique_keys_and_enumerate_dict(gan_train_data, 1)\n",
    "#service = get_unique_keys_and_enumerate_dict(gan_train_data, 2)\n",
    "#flag =  get_unique_keys_and_enumerate_dict(gan_train_data, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe76469-afd7-4065-8ba9-3c0b1f3b1cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize protocol_types\n",
    "for value in test_data[1]: \n",
    "    #print(value)\n",
    "    #print(protocol_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        test_data[1] = test_data[1].replace(value, protocol_types[value])\n",
    "        print(f\"Change {value} to {protocol_types[value]}\")\n",
    "    else:\n",
    "        print(value)\n",
    "\n",
    "# Numericalize protocol_types\n",
    "for value in easy_test_data[1]: \n",
    "    #print(value)\n",
    "    #print(protocol_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        easy_test_data[1] = easy_test_data[1].replace(value, protocol_types[value])\n",
    "        print(f\"Change {value} to {protocol_types[value]}\")\n",
    "    else:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5badb-f5a2-4700-a66b-bbca6c38d818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize service\n",
    "for value in test_data[2]: \n",
    "    #print(value)\n",
    "    #print(service[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        test_data[2] = test_data[2].replace(value, service[value])\n",
    "        print(f\"Change {value} to {service[value]}\")\n",
    "    else:\n",
    "        print(value)\n",
    "        \n",
    "# Numericalize service\n",
    "for value in easy_test_data[2]: \n",
    "    #print(value)\n",
    "    #print(service[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        easy_test_data[2] = easy_test_data[2].replace(value, service[value])\n",
    "        print(f\"Change {value} to {service[value]}\")\n",
    "    else:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2b0a3-eb7b-4981-b100-37ff89028a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize flag\n",
    "for value in test_data[3]: \n",
    "    #print(value)\n",
    "    #print(flag[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        test_data[3] = test_data[3].replace(value, flag[value])\n",
    "        print(f\"Change {value} to {flag[value]}\")\n",
    "    else:\n",
    "        print(value)\n",
    "\n",
    "# Numericalize flag\n",
    "for value in easy_test_data[3]: \n",
    "    #print(value)\n",
    "    #print(flag[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        easy_test_data[3] = easy_test_data[3].replace(value, flag[value])\n",
    "        print(f\"Change {value} to {flag[value]}\")\n",
    "    else:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd1f45-5c2d-4e3f-89f4-9f68f919cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate distinct values in col and add to dict \n",
    "test_attack_types = get_unique_keys_and_enumerate_dict(test_data, 41)\n",
    "test_attack_types_binary = test_attack_types.copy()\n",
    "for key in test_attack_types_binary:\n",
    "    if key != 'normal':\n",
    "        test_attack_types_binary[key] = 1\n",
    "    else:\n",
    "        test_attack_types_binary[key] = 0\n",
    "print(test_attack_types_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6929a19-34b4-4545-93c9-605c103b149f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numericalize class\n",
    "for value in test_data[41]: \n",
    "    #print(value)\n",
    "    #print(attack_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        test_data[41] = test_data[41].replace(value, test_attack_types_binary[value])\n",
    "        print(f\"Change {value} to {test_attack_types_binary[value]}\")\n",
    "    else:\n",
    "        print(value)\n",
    "\n",
    "# Numericalize class\n",
    "for value in easy_test_data[41]: \n",
    "    #print(value)\n",
    "    #print(attack_types[value])\n",
    "    if type(value) != type(1) and type(value) != type(0.1):\n",
    "        easy_test_data[41] = easy_test_data[41].replace(value, test_attack_types_binary[value])\n",
    "        print(f\"Change {value} to {test_attack_types_binary[value]}\")\n",
    "    else:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de99dbf-0331-4b1a-ba65-877811056f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce float64 datatype for all columns data\n",
    "for col in test_data.columns: \n",
    "    test_data[col] = test_data[col]\n",
    "test_data.astype('float64')\n",
    "\n",
    "# Enforce float64 datatype for all columns data\n",
    "for col in easy_test_data.columns: \n",
    "    easy_test_data[col] = easy_test_data[col]\n",
    "easy_test_data.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6887816-9c36-428b-b620-b9acbde2333e",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775a418-e42d-43af-9f2f-ff889fba76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "col_to_exclude = 'nevermind dont exclude'\n",
    "\n",
    "for col in test_data:\n",
    "    if col == col_to_exclude: \n",
    "        continue\n",
    "    test_data[col] = scaler.fit_transform(test_data[[col]])\n",
    "\n",
    "for col in easy_test_data: \n",
    "    if col == col_to_exclude: \n",
    "        continue\n",
    "    easy_test_data[col] = scaler.fit_transform(easy_test_data[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30021f8c-b782-4df9-8cd7-d44afa4d22b5",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729279f-0eff-474c-92b5-045121c9764e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of all curr datasets\n",
    "test_dataframes = [test_data, easy_test_data]\n",
    "\n",
    "test_data_rows = 22544\n",
    "test_data_columns = 42 #does not include class column = total_col - 1\n",
    "\n",
    "easy_test_data_rows = 11850\n",
    "easy_test_data_columns = 42 #does not include class column = total_col - 1\n",
    "\n",
    "# Init target tensors\n",
    "test_data_targets = torch.zeros(1, test_data_rows, dtype=torch.double)\n",
    "easy_test_data_targets = torch.zeros(1, easy_test_data_rows, dtype=torch.double)\n",
    "\n",
    "testing_target_tensors = [test_data_targets, easy_test_data_targets]\n",
    "\n",
    "# List of features intended to be dropped\n",
    "test_data_inputs_cols_to_drop = [42, 40, 39, 36, 35, 34, 33, 30, 29, 27, 26, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 0]\n",
    "easy_test_data_inputs_cols_to_drop = [42, 40, 39, 36, 35, 34, 33, 30, 29, 27, 26, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 0]\n",
    "\n",
    "cols_to_drop = [test_data_inputs_cols_to_drop, easy_test_data_inputs_cols_to_drop]\n",
    "\n",
    "# Init data tensors\n",
    "test_data_inputs = torch.zeros(test_data_columns-len(test_data_inputs_cols_to_drop), test_data_rows, dtype=torch.double)\n",
    "easy_test_data_inputs = torch.zeros(easy_test_data_columns-len(easy_test_data_inputs_cols_to_drop), easy_test_data_rows, dtype=torch.double)\n",
    "\n",
    "testing_input_tensors = [test_data_inputs, easy_test_data_inputs]\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "i=0\n",
    "class_col_name = 41\n",
    "for df in test_dataframes:\n",
    "    # insert class values into initialized targets tensor\n",
    "    numpy_array = df[class_col_name].values\n",
    "    testing_target_tensors[i] = torch.from_numpy(numpy_array).double()\n",
    "    #print(testing_target_tensors[i])  \n",
    "    # Drop classification and create target tensor\n",
    "    df_after_drop = df.drop(columns=[class_col_name])\n",
    "    # Drop rest of cols in cols_to_drop\n",
    "    df_after_drop = df_after_drop.drop(columns=cols_to_drop[i])\n",
    "    # insert remaining col values into initialized training_samples tensor\n",
    "    numpy_array = df_after_drop.values\n",
    "    testing_input_tensors[i] = torch.from_numpy(numpy_array).double()\n",
    "    #print(testing_input_tensors[i])  \n",
    "    i+=1\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "for target_tensors in testing_target_tensors:\n",
    "    attack=0\n",
    "    normal=0\n",
    "    #print(target_tensors)\n",
    "    for value in target_tensors:\n",
    "        #print(value.item())\n",
    "        if value.item() > .015: #value.item() > .10 and value.item() < .12\n",
    "            value.fill_(1.0)\n",
    "            attack+=1\n",
    "        else:\n",
    "            value.fill_(0.0)\n",
    "            normal+=1\n",
    "    print(f'attack: {attack}, normal: {normal}')\n",
    "    #print(target_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ee755-8a85-486a-8f23-71bc6a90f6d7",
   "metadata": {},
   "source": [
    "## Model Trained with Real Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e23f82-7744-47bc-95b8-83cc07d4e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "#DNN eval mode\n",
    "dnn_discriminator_real_data.eval()\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "print('Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_real_data(testing_input_tensors[0].float()) > 0.015).float()\n",
    "true_class = testing_target_tensors[0]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_test_data_accuracy}')\n",
    "print(f'precision: {dnn_test_data_precision}')\n",
    "print(f'recall: {dnn_test_data_recall}')\n",
    "print(f'f1: {dnn_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_test_data_conf_matrix}')\n",
    "\n",
    "# Save results\n",
    "with open('dnn_real_data_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_test_data_conf_matrix}\\n')\n",
    "\n",
    "\n",
    "print('Easy Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_real_data(testing_input_tensors[1].float()) > 0.5).float()\n",
    "true_class = testing_target_tensors[1]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_easy_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_easy_test_data_accuracy}')\n",
    "print(f'precision: {dnn_easy_test_data_precision}')\n",
    "print(f'recall: {dnn_easy_test_data_recall}')\n",
    "print(f'f1: {dnn_easy_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_easy_test_data_conf_matrix}')\n",
    "\n",
    "# Save model and results\n",
    "with open('dnn_real_data_easy_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_easy_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_easy_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_easy_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_easy_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_easy_test_data_conf_matrix}\\n')\n",
    "\n",
    "#RNN just need to change dnn_pred to rnn_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629a2ef-2dfb-4be5-a5ac-dbc711b434cb",
   "metadata": {},
   "source": [
    "## Mixed Data Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45db56-741d-4027-bdcf-db3351decd42",
   "metadata": {},
   "source": [
    "### CT GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2388b48-f46f-42d3-91ba-5e275e3af4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "#DNN eval mode\n",
    "dnn_discriminator_ctGan_synthetic_data.eval()\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "print('Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_ctGan_synthetic_data(testing_input_tensors[0].float()) > 0.015).float()\n",
    "true_class = testing_target_tensors[0]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_test_data_accuracy}')\n",
    "print(f'precision: {dnn_test_data_precision}')\n",
    "print(f'recall: {dnn_test_data_recall}')\n",
    "print(f'f1: {dnn_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_test_data_conf_matrix}')\n",
    "\n",
    "# Save results\n",
    "with open('dnn_ctGan_synthetic_data_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_test_data_conf_matrix}\\n')\n",
    "\n",
    "\n",
    "print('Easy Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_ctGan_synthetic_data(testing_input_tensors[1].float()) > 0.015).float()\n",
    "true_class = testing_target_tensors[1]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_easy_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_easy_test_data_accuracy}')\n",
    "print(f'precision: {dnn_easy_test_data_precision}')\n",
    "print(f'recall: {dnn_easy_test_data_recall}')\n",
    "print(f'f1: {dnn_easy_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_easy_test_data_conf_matrix}')\n",
    "\n",
    "# Save model and results\n",
    "with open('dnn_ctGan_synthetic_data_easy_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_easy_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_easy_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_easy_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_easy_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_easy_test_data_conf_matrix}\\n')\n",
    "\n",
    "#RNN just need to change dnn_pred to rnn_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ec7c9-c0e3-464a-9491-0acfb5ca5755",
   "metadata": {},
   "source": [
    "### coupla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c97c00-2bbe-413c-99da-8a5e6862f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "#DNN eval mode\n",
    "dnn_discriminator_couplaGan_synthetic_data.eval()\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "print('Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_couplaGan_synthetic_data(testing_input_tensors[0].float()) > 0.015).float()\n",
    "true_class = testing_target_tensors[0]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_test_data_accuracy}')\n",
    "print(f'precision: {dnn_test_data_precision}')\n",
    "print(f'recall: {dnn_test_data_recall}')\n",
    "print(f'f1: {dnn_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_test_data_conf_matrix}')\n",
    "\n",
    "# Save results\n",
    "with open('dnn_couplaGan_synthetic_data_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_test_data_conf_matrix}\\n')\n",
    "\n",
    "\n",
    "print('Easy Testing Data')\n",
    "with torch.no_grad():\n",
    "    dnn_pred = (dnn_discriminator_couplaGan_synthetic_data(testing_input_tensors[1].float()) > 0.015).float()\n",
    "true_class = testing_target_tensors[1]\n",
    "\n",
    "#print(dnn_pred)\n",
    "#print(true_class)\n",
    "\n",
    "dnn_easy_test_data_accuracy = accuracy_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_precision = precision_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_recall = recall_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_f1 = f1_score(true_class, dnn_pred)\n",
    "dnn_easy_test_data_conf_matrix = confusion_matrix(true_class, dnn_pred)\n",
    "\n",
    "print(f'accuracy: {dnn_easy_test_data_accuracy}')\n",
    "print(f'precision: {dnn_easy_test_data_precision}')\n",
    "print(f'recall: {dnn_easy_test_data_recall}')\n",
    "print(f'f1: {dnn_easy_test_data_f1}')\n",
    "print(f'conf_matrix: {dnn_easy_test_data_conf_matrix}')\n",
    "\n",
    "# Save model and results\n",
    "with open('dnn_couplaGan_synthetic_data_easy_test_data_results.txt', 'w') as f:\n",
    "    # Write the variable values into the file\n",
    "    f.write(f'accuracy: {dnn_easy_test_data_accuracy}\\n')\n",
    "    f.write(f'precision: {dnn_easy_test_data_precision}\\n')\n",
    "    f.write(f'recall: {dnn_easy_test_data_recall}\\n')\n",
    "    f.write(f'f1: {dnn_easy_test_data_f1}\\n')\n",
    "    f.write(f'conf_matrix: {dnn_easy_test_data_conf_matrix}\\n')\n",
    "\n",
    "#RNN just need to change dnn_pred to rnn_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb52f12-fd5f-46b3-91a4-595932b63677",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22d988-81ea-4013-93e9-d946cdf9565f",
   "metadata": {},
   "source": [
    "Papers: \n",
    "\n",
    "[[DRL-GAN A Hybrid Approach for Binary and Multiclass Network Intrusion Detection - Jan 2023.pdf]]\n",
    "\n",
    "[[AN EFFICIENT DEEP LEARNING APPROACH FOR NETWORK INTRUSION DETECTION SYSTEM ON SOFTWARE DEFINED NETWORK - Jul 2022.pdf]]\n",
    "\n",
    "Dataset: \n",
    "\n",
    "[NSL-KDD HTML Readme](https://storage.googleapis.com/kagglesdsdata/datasets/174616/394223/index.html?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20231208%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20231208T043702Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=8779b5b417b57e7b927a4fc84c28a0c7c06cc76a68af5eb4c57bb27afe04f6d3344e20f7c99faafcef2605d925a8798e2d7a46a8452822f87b8b08c828d46ea121d58c7a766a936522deaaae69c1dcfa863dde452cce8a18466fcd8e9d77618fcc76a361ec656ed0717ea833554b8441c4a6dbd812d70fc5201796149fe7a41a42033d5e86bed7e1c57fdaca3f7762897b97a6e4d5c054e399de79425287b538e05ccdc75ce8344fd996be6f56bd4914dacb25c45faccdd3bcab6eb9fe2300cf62352882432a5cac8fa7726edc6d334e9b88bed36596489bbc32cb8ace309d31ee137a2ad20c28bc7c38d758517422deaa12f92e9836fe571c9b189b1ba0b95b)\n",
    "\n",
    "[NSL-KDD Kaggle Datacard](https://www.kaggle.com/datasets/hassan06/nslkdd?select=KDDTrain%2B.txt)\n",
    "\n",
    "[SKLearn Preprocessing library Normalization MinMaxScaler Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "General Models:\n",
    "\n",
    "[Making Models Pytorch Official Youtube Tutorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html)\n",
    "\n",
    "[Construct and Train Deep learning model Pytorch Docs](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "[torch.nn Documentation](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
    "\n",
    "[pandas.dataframe Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
    "\n",
    "[numpy.ndarray Documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)\n",
    "\n",
    "GAN:\n",
    "\n",
    "[SDV CT GAN Documentation](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers/ctgansynthesizer)\n",
    "\n",
    "[SDV coupla GAN Documentation](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers/copulagansynthesizer)\n",
    "\n",
    "[Conditional sampling SDV Documentation](https://docs.sdv.dev/sdv/single-table-data/sampling/conditional-sampling)\n",
    "\n",
    "[SingleTable Metadata Documentation](https://docs.sdv.dev/sdv/single-table-data/data-preparation/single-table-metadata-api)\n",
    "\n",
    "Other: \n",
    "\n",
    "[Jupyter notebooks shortcuts](https://noteable.io/blog/jupyter-notebook-shortcuts-boost-productivity/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
